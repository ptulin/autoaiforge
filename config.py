"""
AutoAIForge â€” Central Configuration
All settings read from environment variables; defaults included for local testing.
"""

import os
from pathlib import Path
from datetime import datetime, timezone

# â”€â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR   = Path(__file__).parent
DATA_DIR   = BASE_DIR / "data"
TOOLS_DIR  = BASE_DIR / "generated_tools"
LOGS_DIR   = BASE_DIR / "logs"

for _d in (DATA_DIR, TOOLS_DIR, LOGS_DIR):
    _d.mkdir(parents=True, exist_ok=True)

# â”€â”€â”€ LLM Providers (free tiers) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GROQ_API_KEY      = os.getenv("GROQ_API_KEY", "")
TOGETHER_API_KEY  = os.getenv("TOGETHER_API_KEY", "")
HF_TOKEN          = os.getenv("HF_TOKEN", "")

# Groq free tier: 14 400 requests/day, 30 RPM, context 32 k tokens
GROQ_BASE_URL     = "https://api.groq.com/openai/v1"
GROQ_MODEL_LARGE  = "llama-3.1-70b-versatile"
GROQ_MODEL_FAST   = "llama-3.1-8b-instant"

# Together.ai free $25 credit (fallback)
TOGETHER_BASE_URL = "https://api.together.xyz/v1"
TOGETHER_MODEL    = "meta-llama/Llama-3.1-70B-Instruct-Turbo"

# HuggingFace Inference API (fallback)
HF_BASE_URL       = "https://api-inference.huggingface.co/models"
HF_MODEL          = "mistralai/Mistral-7B-Instruct-v0.3"

# â”€â”€â”€ Data Sources â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
YOUTUBE_API_KEY   = os.getenv("YOUTUBE_API_KEY", "")
YOUTUBE_MAX_RESULTS_PER_QUERY = 50
YOUTUBE_QUOTA_BUDGET = 9000   # Stay below 10 000/day limit

# Keywords searched every run (YouTube + RSS)
SEARCH_KEYWORDS = [
    "AI news",
    "Claude AI updates",
    "open source AI tools",
    "LLM advancements",
    "Claude Code",
    "artificial intelligence breakthrough",
    "machine learning tools",
    "AI automation",
    "Anthropic Claude",
    "AI coding assistant",
    "open source LLM",
    "AI agent framework",
    "vector database AI",
    "RAG retrieval augmented generation",
    "multimodal AI",
    "AI safety news",
    "Groq inference",
    "local LLM",
    "AI productivity tools",
    "generative AI",
]

# Reddit subreddits to scrape (no auth needed for public feeds)
REDDIT_SUBREDDITS = [
    "MachineLearning",
    "artificial",
    "LocalLLaMA",
    "ClaudeAI",
    "singularity",
    "AItools",
    "learnmachinelearning",
]

# â”€â”€â”€ Storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SQLITE_PATH   = str(DATA_DIR / "news.db")
CHROMA_PATH   = str(DATA_DIR / "chroma_db")

NEWS_RETENTION_DAYS = 30      # Keep news for 30 days
EMBED_MODEL         = "all-MiniLM-L6-v2"   # Lightweight, fast, 384-dim

# â”€â”€â”€ Analysis Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOP_TOPICS_COUNT    = 10      # Topics to identify per run
IDEAS_PER_TOPIC     = 3       # Tool ideas per topic
MAX_TOOLS_PER_RUN   = 5       # Cap tool generation to avoid quota burn

# â”€â”€â”€ Developer Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_CORRECTION_LOOPS = 5      # LLM self-correction attempts
TEST_TIMEOUT_SECONDS = 60     # Max time for test suite
SANDBOX_DIR          = DATA_DIR / "sandbox"
SANDBOX_DIR.mkdir(exist_ok=True)

# â”€â”€â”€ GitHub Publishing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GITHUB_TOKEN        = os.getenv("GITHUB_TOKEN", "")
GITHUB_USERNAME     = os.getenv("GITHUB_USERNAME", "")
TOOLS_REPO_NAME     = os.getenv("TOOLS_REPO_NAME", "autoaiforge-tools")
TOOLS_REPO_DESC     = "ðŸ¤– Daily AI tools auto-generated by AutoAIForge"

# â”€â”€â”€ Notifications (optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WEBHOOK_URL         = os.getenv("WEBHOOK_URL", "")   # Slack/Discord webhook

# â”€â”€â”€ Runtime â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RUN_DATE = datetime.now(timezone.utc).strftime("%Y-%m-%d")
RUN_TS   = datetime.now(timezone.utc).isoformat()

def validate() -> list[str]:
    """Return list of missing critical config items."""
    missing = []
    if not GROQ_API_KEY and not TOGETHER_API_KEY and not HF_TOKEN:
        missing.append("At least one LLM key required: GROQ_API_KEY | TOGETHER_API_KEY | HF_TOKEN")
    if not GITHUB_TOKEN:
        missing.append("GITHUB_TOKEN (provided automatically by GitHub Actions)")
    return missing
